# sirius_contest
Решение контеста Т-банка в рамках отбора на смену в Сириусе по ML

## Описание файлов

* `submission.csv` - финальный файл с предсказаниями
* `solution_sirius_final.ipynb` - ноутбук с решением (также его можно найти на colab, [тык](https://colab.research.google.com/drive/1xMAh0iJvb2D4Raj2HP1Vcq5C96SFvDH8?usp=sharing)
* `label_encoder_7824.pkl` - веса лучшей модели

## Шаги решения

### Разметка

По условию нельзя использовать api, только опенсорсные модели

При выборе я опирался на этот [бенчмарк](https://llmarena.ru/?leaderboard&utm_source=pollux&utm_medium=cpc&utm_campaign=pollux) на русском языке

Основным критерием было - модель должна (почти) помещаться на T4, иначе длины сессии и ресурсов Colab могло не хватить для всех операций. В среднем на разметку одного отзыва уходило менее 5 секунд. Пропускная способность шины для перегонки данных с CPU на GPU и обратно оказалась достаточной.

Так я выбрал `T-lite-1.0`.

Также были эксперименты с моделями  qwen и llama меньших размеров (1.5-4b), в том числе инструктивными, которые показали плохие результаты. Даже у инструктивных моделей на выходе получалось в 2-3 раза больше категорий ("надевуха", "тексtile" и проч.)

### Аугментация и train/test split

У классов наблюдался сильный дисбаланс. Максимальная встречаемость была более 1200, минимальная - всего 2

Было принято непростое решение:
* изменить распределение классов (наиболее редкий размножить в 10 раз, топ-2 по редкости - в 9 раз, и т.д. до 2)
* разделить датасет на train/test **после** аугментации
* использовать стратификацию по классам

**Очевидные риски**: data leakage, переобучение

Обоснование:
* по условию размечать test-выборку было нельзя, а train-выборка была достаточно маленькой (примерно в 4 раза меньше)
* с учетом того, что после разметки один из классов встретился всего 2 раза, был риск, что f1-score на нем будет нулем, так как модель просто его не запомнит, либо он не встретится в test
* именно поэтому я не стал делать `split` до аугментации
* при визуальном изучении нескольких **случайных** подвыборок сгенерированных отзывов был сделан вывод, что синтезированные отзывы имеют существенные отличия в сравнении с оригиналами, поэтому была допущена идея делать `split` после аугментации

Аугментация проводилась с помощью `Qwen-2.5-1.5b-Instruct`, дообученного под русский командой Vikhr

При аугментации я использовал промпт, предлагающий модели, используя максимально отличную от оригинала лексику, синтезировать отзыв, близкий по смыслу и эмоциональному фону.

Самое главное - была добавлена **LLM-as-a-judge** поверх синтезированных отзывов (использовался тот же `Qwen`, что и на генерации). Нам лучше потратить больше попыток на генерацию, чем пропустить в train какую-то ерунду.

Принцип **LLM-as-a-judge** был следующий:
* модель генерирует отзыв
* эта же модель с нужным промптом с ненулевой температурой вызывается **дважды** (это быстро), чтобы ответить на вопрос - это отзыв или отказ в генерации
* при условии обоих результатов = `True` результат генерации сохранялся
* если хоть одно `False`, генерация отзыва шла повторно, пока не будет достигнуто нужное количество

В ходе опытов стало понятно, что основная проблема - когда модель пишет, что не может выполнить запрос, это и пришлось фильтровать.

### Обучение 

Я использовал подход LoRA, применив библиотеку peft. 

За основу взял `ruRoberta-large`

Модель взял исходя из этого [лидерборда](https://russiansuperglue.com/leaderboard/2)

Ключевыми критериями было - BERT-подобная архитектура, небольшое количество параметров при условии высокого скора. В итоге взял модель от Сбера

### Выводы

Примерно после 2000 шага лосс начал снова расти, качество на тесте падать. Итого небольшой data leakage почти не повлиял на обучение LoRA.

Я взял модель с этого лучшего чекпоинта и получил `f1-weightened` порядка **0.782**

Стоит учитывать, что здесь распределение классов отличается от исходного. Увеличить количество сразу всех отзывов раз в 10 или больше я не мог по причине ограниченности времени и вычислительных ресурсов, поэтому пришлось менять баланс.
